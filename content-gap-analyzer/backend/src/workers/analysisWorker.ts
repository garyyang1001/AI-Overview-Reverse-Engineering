import { Worker, Job } from 'bullmq';
import IORedis from 'ioredis';
import { AnalysisJobData } from '../services/queueService';
import logger from '../utils/logger';

// å°å…¥å„éšæ®µæœå‹™
import { serpApiService } from '../services/serpApiService';
import { playwrightService } from '../services/playwrightService';
import { contentRefinementService } from '../services/contentRefinementService';
import { openaiService } from '../services/geminiService';

// å°å…¥éŒ¯èª¤è™•ç†ç³»çµ±
import { errorHandler } from '../services/errorHandler';
import { WorkerStepError } from '../types/errors';

/**
 * åˆ†æ Worker - è™•ç†å®Œæ•´çš„ 5 éšæ®µåˆ†æå·¥ä½œæµ
 */
class AnalysisWorker {
  private worker: Worker<AnalysisJobData>;
  private redisConnection: IORedis;

  constructor() {
    // å‰µå»ºå°ˆç”¨çš„ Redis é€£æ¥çµ¦ Worker
    this.redisConnection = new IORedis({
      host: process.env.REDIS_HOST || 'localhost',
      port: parseInt(process.env.REDIS_PORT || '6379'),
      maxRetriesPerRequest: null,
      enableReadyCheck: false,
    });

    this.worker = new Worker<AnalysisJobData>(
      'analysis',
      this.processAnalysisJob.bind(this),
      {
        connection: this.redisConnection,
        concurrency: 2, // åŒæ™‚è™•ç† 2 å€‹ä»»å‹™
        limiter: {
          max: 10,      // æ¯ 10 ç§’æœ€å¤šè™•ç† 10 å€‹ä»»å‹™
          duration: 10000,
        },
      }
    );

    this.setupEventListeners();
  }

  /**
   * è¨­ç½®äº‹ä»¶ç›£è½å™¨
   */
  private setupEventListeners(): void {
    this.worker.on('ready', () => {
      logger.info('Analysis worker is ready and waiting for jobs');
    });

    this.worker.on('error', (error) => {
      logger.error('Analysis worker error:', error);
    });

    this.worker.on('failed', (job, err) => {
      logger.error(`Job ${job?.id} failed:`, err);
    });

    this.worker.on('completed', (job) => {
      logger.info(`Job ${job.id} completed successfully`);
    });
  }

  /**
   * è™•ç†åˆ†æä»»å‹™çš„ä¸»è¦å‡½æ•¸
   */
  private async processAnalysisJob(job: Job<AnalysisJobData>): Promise<any> {
    const { jobId, targetKeyword, userPageUrl, competitorUrls } = job.data;
    
    logger.info(`Starting analysis job ${jobId} for keyword: ${targetKeyword}`);

    try {
      // åˆå§‹åŒ–è™•ç†æ­¥é©Ÿç‹€æ…‹å’ŒéŒ¯èª¤è¿½è¹¤
      const processingSteps = {
        serpApiStatus: 'pending',
        userPageStatus: 'pending',
        competitorPagesStatus: 'pending',
        contentRefinementStatus: 'pending',
        aiAnalysisStatus: 'pending'
      };

      const completedSteps: string[] = [];
      const stepErrors: WorkerStepError[] = [];
      const stepWarnings: WorkerStepError[] = [];

      // éšæ®µ 1: AIO æ•¸æ“šæå– (10%)
      await job.updateProgress(10);
      processingSteps.serpApiStatus = 'processing';
      
      logger.info(`Job ${jobId}: Fetching AI Overview for keyword: ${targetKeyword}`);
      
      let aiOverview;
      try {
        aiOverview = await serpApiService.getAIOverview(targetKeyword);
        
        if (!aiOverview) {
          processingSteps.serpApiStatus = 'failed';
          const stepError = errorHandler.createWorkerStepError(
            'serpapi',
            'SERPAPI_FAILED',
            'No search data available for the given keyword',
            [],
            false
          );
          stepErrors.push(stepError);
          throw new Error('No search data available for the given keyword');
        }
        
        processingSteps.serpApiStatus = 'completed';
        completedSteps.push('serpapi');
        
      } catch (error: any) {
        processingSteps.serpApiStatus = 'failed';
        const stepError = errorHandler.createWorkerStepError(
          'serpapi',
          'SERPAPI_FAILED',
          error.message,
          [],
          false
        );
        stepErrors.push(stepError);
        throw error;
      }
      
      await job.updateProgress(30);

      // éšæ®µ 2: æ‰¹é‡å…§å®¹çˆ¬å– (30-60%)
      logger.info(`Job ${jobId}: Starting content scraping phase`);
      processingSteps.userPageStatus = 'processing';
      processingSteps.competitorPagesStatus = 'processing';

      // çˆ¬å–ç”¨æˆ¶é é¢
      let userPage;
      try {
        const userPageResult = await playwrightService.scrapePage(userPageUrl);
        
        if (!userPageResult.success) {
          processingSteps.userPageStatus = 'failed';
          
          const playwrightError = errorHandler.classifyPlaywrightError(
            userPageResult.error || 'CONTENT_ERROR',
            userPageUrl,
            userPageResult.errorDetails || 'Unknown error'
          );
          
          const stepError = errorHandler.createWorkerStepError(
            'user_scraping',
            'SCRAPING_FAILED',
            `Failed to scrape user's page: ${userPageResult.error}`,
            [playwrightError],
            true  // Changed to true - allow continuation with fallback
          );
          stepErrors.push(stepError);
          
          logger.warn(`ğŸ”„ [WORKER] User page scraping failed, will use Gemini URL Context fallback:`, {
            url: userPageUrl,
            error: userPageResult.error,
            details: userPageResult.errorDetails
          });
          
          // Create fallback user page object with URL only (for Gemini URL Context)
          userPage = {
            url: userPageResult.url,
            title: '',
            headings: [],
            cleanedContent: '', // Empty content signals fallback needed
            metaDescription: '',
            scrapeFailure: {
              error: userPageResult.error,
              details: userPageResult.errorDetails,
              needsFallback: true
            }
          };
        } else {
          // è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥å…¼å®¹å¾ŒçºŒè™•ç†
          userPage = {
            url: userPageResult.url,
            title: userPageResult.title || '',
            headings: userPageResult.headings || [],
            cleanedContent: userPageResult.content || '',
            metaDescription: userPageResult.metaDescription || ''
          };
          
          processingSteps.userPageStatus = 'completed';
          completedSteps.push('user_scraping');
        }
        
      } catch (error: any) {
        processingSteps.userPageStatus = 'failed';
        if (!stepErrors.find(e => e.step === 'user_scraping')) {
          const stepError = errorHandler.createWorkerStepError(
            'user_scraping',
            'SCRAPING_FAILED',
            error.message,
            [],
            true  // Changed to true - allow continuation with fallback
          );
          stepErrors.push(stepError);
        }
        
        logger.warn(`ğŸ”„ [WORKER] Exception during user page scraping, will use Gemini URL Context fallback:`, {
          url: userPageUrl,
          error: error.message
        });
        
        // Create fallback user page object for unexpected errors
        userPage = {
          url: userPageUrl,
          title: '',
          headings: [],
          cleanedContent: '', // Empty content signals fallback needed
          metaDescription: '',
          scrapeFailure: {
            error: 'EXCEPTION',
            details: error.message,
            needsFallback: true
          }
        };
      }
      
      await job.updateProgress(40);

      // æº–å‚™ç«¶çˆ­è€… URL åˆ—è¡¨
      const searchReferences = aiOverview.references || [];
      const additionalCompetitorUrls = competitorUrls || [];
      const allCompetitorUrls = [
        ...searchReferences.slice(0, 10),
        ...additionalCompetitorUrls
      ];
      const uniqueCompetitorUrls = [...new Set(allCompetitorUrls)]
        .filter(url => url !== userPageUrl);

      // æ‰¹é‡çˆ¬å–ç«¶çˆ­è€…é é¢
      let competitorPages: any[] = [];
      try {
        const competitorResults = await playwrightService.scrapeMultiplePages(uniqueCompetitorUrls);
        const successfulResults = competitorResults.filter(result => result.success);
        const failedResults = competitorResults.filter(result => !result.success);
        
        // è½‰æ›æˆåŠŸçš„çµæœç‚ºèˆŠæ ¼å¼
        competitorPages = successfulResults.map(result => ({
          url: result.url,
          title: result.title || '',
          headings: result.headings || [],
          cleanedContent: result.content || '',
          metaDescription: result.metaDescription || ''
        }));
        
        // è¨˜éŒ„å¤±æ•—çš„é é¢
        if (failedResults.length > 0) {
          logger.warn(`Job ${jobId}: ${failedResults.length}/${competitorResults.length} competitor pages failed to scrape`);
          failedResults.forEach(result => {
            logger.warn(`Failed to scrape ${result.url}: ${result.error} - ${result.errorDetails}`);
          });
        }
        
        if (competitorPages.length === 0) {
          processingSteps.competitorPagesStatus = 'failed';
          logger.warn(`Job ${jobId}: No competitor pages successfully scraped`);
        } else if (failedResults.length > 0) {
          processingSteps.competitorPagesStatus = 'partial';
        } else {
          processingSteps.competitorPagesStatus = 'completed';
        }
      } catch (error: any) {
        logger.warn(`Job ${jobId}: Competitor scraping failed: ${error.message}`);
        processingSteps.competitorPagesStatus = 'failed';
        competitorPages = [];
      }

      await job.updateProgress(60);

      // éšæ®µ 3: å…§å®¹ç²¾ç…‰ (60-80%)
      logger.info(`Job ${jobId}: Starting content refinement phase`);
      processingSteps.contentRefinementStatus = 'processing';

      const allPages = [userPage, ...competitorPages];
      let refinedContents: any[];
      let refinementSuccessful = true;

      try {
        refinedContents = await contentRefinementService.refineMultiplePages(allPages, jobId);
        processingSteps.contentRefinementStatus = 'completed';
      } catch (error: any) {
        logger.warn(`Job ${jobId}: Content refinement failed: ${error.message}`);
        processingSteps.contentRefinementStatus = 'failed';
        refinementSuccessful = false;
        
        // é™ç´šï¼šä½¿ç”¨åŸå§‹å…§å®¹
        refinedContents = allPages.map(page => ({
          url: page.url,
          originalLength: page.cleanedContent?.length || 0,
          refinedSummary: page.cleanedContent || '',
          keyPoints: [],
          refinementSuccess: false,
          refinementStats: { totalChunks: 1, successful: 0, failed: 1 }
        }));
      }

      const refinedUserContent = refinedContents[0];
      const refinedCompetitorContents = refinedContents.slice(1);

      await job.updateProgress(80);

      // éšæ®µ 4: æœ€çµ‚å·®è·åˆ†æ (80-95%)
      logger.info(`Job ${jobId}: Performing content gap analysis`);
      processingSteps.aiAnalysisStatus = 'processing';

      let analysisResult: any;

      try {
        const validCompetitorContents = refinedCompetitorContents.filter((_, index) => 
          competitorPages[index] && competitorPages[index].cleanedContent
        );

        // è©³ç´°è¨˜éŒ„å‚³éçµ¦ OpenAI çš„æ•¸æ“š
        logger.info(`Job ${jobId}: Preparing data for OpenAI analysis`, {
          targetKeyword,
          userPageUrl: userPage.url,
          userContentLength: refinedUserContent.refinedSummary?.length || 0,
          competitorCount: validCompetitorContents.length,
          aiOverviewLength: aiOverview.summaryText?.length || 0
        });

        // è¨˜éŒ„ç”¨æˆ¶é é¢ç²¾ç…‰æ‘˜è¦çš„å‰100å­—ç¬¦
        if (refinedUserContent.refinedSummary) {
          logger.debug(`Job ${jobId}: User page refined summary preview:`, {
            preview: refinedUserContent.refinedSummary.substring(0, 100) + '...',
            fullLength: refinedUserContent.refinedSummary.length
          });
        }

        // è¨˜éŒ„ç«¶çˆ­å°æ‰‹ç²¾ç…‰æ‘˜è¦
        validCompetitorContents.forEach((refined, index) => {
          logger.debug(`Job ${jobId}: Competitor ${index} refined summary:`, {
            url: competitorPages[index]?.url,
            preview: refined.refinedSummary?.substring(0, 100) + '...',
            fullLength: refined.refinedSummary?.length || 0
          });
        });

        analysisResult = await openaiService.analyzeContentGap({
          targetKeyword,
          userPage: {
            ...userPage,
            cleanedContent: refinedUserContent.refinedSummary
          },
          aiOverview,
          competitorPages: validCompetitorContents.map((refined, index) => ({
            ...competitorPages[index],
            cleanedContent: refined.refinedSummary
          })),
          jobId  // v5.1 æ–°å¢ï¼šå‚³é jobId ç”¨æ–¼æˆæœ¬è¿½è¹¤
        });

        processingSteps.aiAnalysisStatus = 'completed';
      } catch (error: any) {
        logger.error(`Job ${jobId}: AI analysis failed: ${error.message}`);
        processingSteps.aiAnalysisStatus = 'failed';
        
        // æä¾›é™ç´šåˆ†æçµæœ
        analysisResult = this.generateFallbackAnalysis(targetKeyword, aiOverview);
      }

      await job.updateProgress(95);

      // éšæ®µ 5: çµæœæº–å‚™èˆ‡å„²å­˜ (95-100%)
      // è©•ä¼°ä»»å‹™å®Œæˆç‹€æ…‹
      const jobCompletion = errorHandler.evaluateJobCompletion(
        completedSteps,
        stepErrors,
        stepWarnings
      );

      // è¨˜éŒ„éŒ¯èª¤çµ±è¨ˆ
      if (stepErrors.length > 0 || stepWarnings.length > 0) {
        errorHandler.logErrorStatistics([...stepErrors, ...stepWarnings]);
      }

      const result = {
        ...analysisResult,
        analysisId: jobId,
        timestamp: new Date().toISOString(),
        aiOverviewData: {
          ...aiOverview,
          dataSource: aiOverview.fallbackUsed ? `${aiOverview.source} (fallback)` : 'AI Overview'
        },
        competitorUrls: uniqueCompetitorUrls,
        processingSteps,
        usedFallbackData: aiOverview.fallbackUsed || false,
        refinementSuccessful,
        
        // v5.1 æ–°å¢ï¼šåˆ†å±¤éŒ¯èª¤è™•ç†çµæœ
        jobCompletion,
        errors: stepErrors.map(error => errorHandler.translateToFrontendError(error, jobId)),
        warnings: stepWarnings.map(warning => errorHandler.translateToFrontendError(warning, jobId)),
        qualityAssessment: {
          score: jobCompletion.qualityScore,
          level: this.getQualityLevel(jobCompletion.qualityScore),
          completedSteps: completedSteps.length,
          totalSteps: 5,
          criticalFailures: stepErrors.filter(e => !e.canContinue).length,
          fallbacksUsed: jobCompletion.fallbacksUsed
        }
      };

      await job.updateProgress(100);

      const statusMessage = jobCompletion.status === 'completed' 
        ? 'completed successfully'
        : `completed with ${jobCompletion.status === 'completed_with_errors' ? 'errors' : 'issues'}`;
      
      logger.info(`Job ${jobId}: Analysis ${statusMessage} (quality: ${jobCompletion.qualityScore})`);
      return result;

    } catch (error: any) {
      logger.error(`Job ${jobId}: Analysis failed:`, error);
      throw error;
    }
  }

  /**
   * ç”Ÿæˆé™ç´šåˆ†æçµæœï¼ˆv5.1 çµæ§‹ï¼‰
   */
  private generateFallbackAnalysis(targetKeyword: string, aiOverview: any): any {
    return {
      executiveSummary: {
        mainReasonForExclusion: `âš ï¸ æŠ€è¡“å•é¡Œå°è‡´åˆ†æéƒ¨åˆ†å®Œæˆã€‚é—œéµå­—ã€Œ${targetKeyword}ã€çš„æœç´¢çµæœ${aiOverview.fallbackUsed ? 'ä½¿ç”¨äº†å‚™ç”¨æ•¸æ“šæº' : 'åŒ…å« AI Overview'}ï¼Œä½†ç„¡æ³•å®Œæˆå®Œæ•´çš„ AI åˆ†æã€‚`,
        topPriorityAction: 'è«‹æª¢æŸ¥ç³»çµ±è¨­å®šä¸¦é‡æ–°å˜—è©¦åˆ†æï¼Œæˆ–æ‰‹å‹•æª¢æŸ¥ç«¶çˆ­å°æ‰‹å…§å®¹ã€‚',
        confidenceScore: 30
      },
      contentGapAnalysis: {
        missingTopics: [
          {
            topic: 'éœ€è¦æ‰‹å‹•åˆ†æ',
            importance: 'high',
            competitorCoverage: 0,
            implementationComplexity: 'unknown',
            description: 'ç”±æ–¼æŠ€è¡“å•é¡Œï¼Œç„¡æ³•å®Œæˆè‡ªå‹•ä¸»é¡Œåˆ†æã€‚è«‹æ‰‹å‹•æª¢æŸ¥ç«¶çˆ­å°æ‰‹é é¢å…§å®¹ã€‚'
          }
        ],
        missingEntities: [
          {
            entity: 'éœ€è¦æ‰‹å‹•è­˜åˆ¥',
            type: 'unknown',
            relevance: 'high',
            competitorMentions: 0,
            description: 'ç„¡æ³•è‡ªå‹•è­˜åˆ¥ç¼ºå¤±å¯¦é«”ã€‚å»ºè­°æª¢æŸ¥ç«¶çˆ­å°æ‰‹æåŠçš„å“ç‰Œã€äººç‰©ã€çµ„ç¹”ç­‰ã€‚'
          }
        ],
        contentDepthGaps: [
          {
            area: 'æ•´é«”å…§å®¹åˆ†æ',
            currentDepth: 'unknown',
            requiredDepth: 'unknown',
            competitorAdvantage: 'ç”±æ–¼æŠ€è¡“å•é¡Œï¼Œç„¡æ³•è©•ä¼°ç«¶çˆ­å°æ‰‹å„ªå‹¢'
          }
        ]
      },
      eatAnalysis: {
        experience: {
          userScore: 50,
          competitorAverage: 50,
          gaps: ['ç„¡æ³•è©•ä¼°'],
          opportunities: ['è«‹é‡æ–°å˜—è©¦åˆ†æ']
        },
        expertise: {
          userScore: 50,
          competitorAverage: 50,
          gaps: ['ç„¡æ³•è©•ä¼°'],
          opportunities: ['è«‹é‡æ–°å˜—è©¦åˆ†æ']
        },
        authoritativeness: {
          userScore: 50,
          competitorAverage: 50,
          gaps: ['ç„¡æ³•è©•ä¼°'],
          opportunities: ['è«‹é‡æ–°å˜—è©¦åˆ†æ']
        },
        trustworthiness: {
          userScore: 50,
          competitorAverage: 50,
          gaps: ['ç„¡æ³•è©•ä¼°'],
          opportunities: ['è«‹é‡æ–°å˜—è©¦åˆ†æ']
        }
      },
      actionablePlan: {
        immediate: [
          {
            action: 'æª¢æŸ¥ç³»çµ±è¨­å®š',
            title: 'æª¢æŸ¥ API è¨­å®š',
            description: 'é©—è­‰ OpenAI API å¯†é‘°æ˜¯å¦æ­£ç¢ºé…ç½®ä¸¦é‡æ–°å˜—è©¦åˆ†æ',
            impact: 'high',
            effort: 'low',
            timeline: '5-10 åˆ†é˜',
            implementation: '1. æª¢æŸ¥ .env æ–‡ä»¶ä¸­çš„ OPENAI_API_KEY 2. é‡å•Ÿå¾Œç«¯æœå‹™ 3. é‡æ–°æäº¤åˆ†æ',
            expectedOutcome: 'ä¿®å¾©æŠ€è¡“å•é¡Œï¼Œç²å¾—å®Œæ•´çš„åˆ†æçµæœ'
          }
        ],
        shortTerm: [
          {
            action: 'æ‰‹å‹•ç«¶çˆ­å°æ‰‹åˆ†æ',
            title: 'æ‰‹å‹•å…§å®¹æ¯”è¼ƒ',
            description: 'åœ¨ç³»çµ±ä¿®å¾©å‰ï¼Œæ‰‹å‹•æ¯”è¼ƒç«¶çˆ­å°æ‰‹å…§å®¹',
            impact: 'medium',
            effort: 'high',
            timeline: '1-2 å°æ™‚',
            implementation: '1. è¨ªå• AI Overview ä¸­çš„ç«¶çˆ­å°æ‰‹é é¢ 2. è¨˜éŒ„å…§å®¹å·®ç•° 3. è­˜åˆ¥ç¼ºå¤±ä¸»é¡Œå’Œå¯¦é«”',
            expectedOutcome: 'ç²å¾—åŸºæœ¬çš„å…§å®¹å·®è·æ´å¯Ÿ'
          }
        ],
        longTerm: []
      },
      competitorInsights: {
        topPerformingCompetitor: {
          url: 'unknown',
          strengths: ['ç„¡æ³•åˆ†æ'],
          keyDifferentiators: ['è«‹é‡æ–°å˜—è©¦åˆ†æ']
        },
        commonPatterns: ['ç”±æ–¼æŠ€è¡“å•é¡Œï¼Œç„¡æ³•è­˜åˆ¥ç«¶çˆ­å°æ‰‹çš„å…±åŒæ¨¡å¼']
      },
      successMetrics: {
        primaryKPI: 'ä¿®å¾©ç³»çµ±å•é¡Œ',
        trackingRecommendations: [
          'æª¢æŸ¥ç³»çµ±æ—¥èªŒç¢ºèªéŒ¯èª¤åŸå› ',
          'é©—è­‰ API é…ç½®',
          'é‡æ–°åŸ·è¡Œåˆ†æ'
        ],
        timeframe: 'ç«‹å³è™•ç†'
      }
    };
  }


  /**
   * ç²å–å“è³ªç­‰ç´šï¼ˆæ–°ç‰ˆæœ¬ï¼‰
   */
  private getQualityLevel(score: number): 'excellent' | 'good' | 'fair' | 'poor' {
    if (score >= 85) return 'excellent';
    if (score >= 70) return 'good';
    if (score >= 50) return 'fair';
    return 'poor';
  }

  /**
   * å„ªé›…é—œé–‰ Worker
   */
  async shutdown(): Promise<void> {
    try {
      await this.worker.close();
      await this.redisConnection.quit();
      logger.info('Analysis worker shutdown completed');
    } catch (error) {
      logger.error('Error during worker shutdown:', error);
    }
  }
}

// å‰µå»ºä¸¦å•Ÿå‹• Worker
const analysisWorker = new AnalysisWorker();

logger.info('Analysis Worker initialized successfully');

export default analysisWorker;