import { Worker, Job } from 'bullmq';
import IORedis from 'ioredis';
import { AnalysisJobData } from '../services/queueService';
import logger from '../utils/logger';

// å°å…¥å„éšæ®µæœå‹™
import { serpApiService } from '../services/serpApiService';
import { crawl4aiService, ScrapeResult } from '../services/crawl4aiService'; // Changed from playwrightService

import { geminiService } from '../services/geminiService';
// import { promptService } from '../services/promptService'; // Not used in this version

// å°å…¥éŒ¯èª¤è™•ç†ç³»çµ±
import { errorHandler } from '../services/errorHandler';
import { WorkerStepError } from '../types/errors';
import { PageContent, AnalysisReport } from '../shared/types'; // Import PageContent

/**
 * åˆ†æ Worker - è™•ç†å®Œæ•´çš„ 4 éšæ®µåˆ†æå·¥ä½œæµ
 */
class AnalysisWorker {
  private worker: Worker<AnalysisJobData>;
  private redisConnection: IORedis;

  constructor() {
    // å‰µå»ºå°ˆç”¨çš„ Redis é€£æ¥çµ¦ Worker
    this.redisConnection = new IORedis({
      host: process.env.REDIS_HOST || 'localhost',
      port: parseInt(process.env.REDIS_PORT || '6379'),
      maxRetriesPerRequest: null,
      enableReadyCheck: false,
    });

    this.worker = new Worker<AnalysisJobData>(
      'analysis',
      this.processAnalysisJob.bind(this),
      {
        connection: this.redisConnection,
        concurrency: 2, // åŒæ™‚è™•ç† 2 å€‹ä»»å‹™
        limiter: {
          max: 10,      // æ¯ 10 ç§’æœ€å¤šè™•ç† 10 å€‹ä»»å‹™
          duration: 10000,
        },
      }
    );

    this.setupEventListeners();
  }

  /**
   * è¨­ç½®äº‹ä»¶ç›£è½å™¨
   */
  private setupEventListeners(): void {
    this.worker.on('ready', () => {
      logger.info('Analysis worker is ready and waiting for jobs');
    });

    this.worker.on('error', (error) => {
      logger.error('Analysis worker error:', error);
    });

    this.worker.on('failed', (job, err) => {
      logger.error(`Job ${job?.id} failed:`, err);
    });

    this.worker.on('completed', (job) => {
      logger.info(`Job ${job.id} completed successfully`);
    });
  }

  /**
   * è™•ç†åˆ†æä»»å‹™çš„ä¸»è¦å‡½æ•¸
   */
  private async processAnalysisJob(job: Job<AnalysisJobData>): Promise<any> {
    const { jobId, targetKeyword, userPageUrl, competitorUrls } = job.data;
    const jobStartTime = Date.now();
    let finalPhaseStartTime = Date.now();
    
    logger.info(`Starting analysis job ${jobId} for keyword: ${targetKeyword}`);

    try {
      // åˆå§‹åŒ–è™•ç†æ­¥é©Ÿç‹€æ…‹å’ŒéŒ¯èª¤è¿½è¹¤
      const processingSteps = {
        serpApiStatus: 'pending',
        userPageStatus: 'pending',
        competitorPagesStatus: 'pending',
        aiAnalysisStatus: 'pending'
      };

      const completedSteps: string[] = [];
      const stepErrors: WorkerStepError[] = [];
      const stepWarnings: WorkerStepError[] = [];

      // éšæ®µ 1: AIO æ•¸æ“šæå– (10%)
      await job.updateProgress(10);
      processingSteps.serpApiStatus = 'processing';
      
      logger.info(`Job ${jobId}: Fetching AI Overview for keyword: ${targetKeyword}`);
      
      let aiOverview;
      try {
        aiOverview = await serpApiService.getAIOverview(targetKeyword);
        
        if (!aiOverview) {
          processingSteps.serpApiStatus = 'failed';
          const stepError = errorHandler.createWorkerStepError(
            'serpapi',
            'SERPAPI_FAILED',
            'No search data available for the given keyword',
            [],
            false
          );
          stepErrors.push(stepError);
          throw new Error('No search data available for the given keyword');
        }
        
        processingSteps.serpApiStatus = 'completed';
        completedSteps.push('serpapi');
        
      } catch (error: any) {
        processingSteps.serpApiStatus = 'failed';
        const stepError = errorHandler.createWorkerStepError(
          'serpapi',
          'SERPAPI_FAILED',
          error.message,
          [],
          false
        );
        stepErrors.push(stepError);
        throw error;
      }
      
      await job.updateProgress(30);

      // éšæ®µ 2: æ‰¹é‡å…§å®¹çˆ¬å– (30-60%)
      logger.info(`Job ${jobId}: Starting content scraping phase`);
      processingSteps.userPageStatus = 'processing';
      processingSteps.competitorPagesStatus = 'processing';

      // çˆ¬å–ç”¨æˆ¶é é¢
      let userPage: PageContent; // Explicitly type userPage
      try {
        const userPageResult: ScrapeResult = await crawl4aiService.scrapePage(userPageUrl); // Use crawl4aiService
        
        if (!userPageResult.success) {
          processingSteps.userPageStatus = 'failed';
          
          const stepError = errorHandler.createWorkerStepError(
            'user_scraping',
            'SCRAPING_FAILED',
            `Failed to scrape user's page: ${userPageResult.error}`,
            [],
            true  // Allow continuation with fallback
          );
          stepErrors.push(stepError);
          
          logger.warn(`ğŸ”„ [WORKER] User page scraping failed, will use Gemini URL Context fallback:`, {
            url: userPageUrl,
            error: userPageResult.error,
            details: userPageResult.errorDetails
          });
          
          // Create fallback user page object with URL only (for Gemini URL Context)
          userPage = {
            url: userPageResult.url,
            title: '',
            headings: [],
            cleanedContent: '', // Empty content signals fallback needed
            metaDescription: '',
            // No scrapeFailure property needed here, handled by stepErrors
          };
        } else {
          // è½‰æ›ç‚º PageContent æ ¼å¼
          userPage = {
            url: userPageResult.url,
            title: userPageResult.title || '',
            headings: userPageResult.headings || [],
            cleanedContent: userPageResult.content || '',
            metaDescription: userPageResult.metaDescription || ''
          };
          
          processingSteps.userPageStatus = 'completed';
          completedSteps.push('user_scraping');
        }
        
      } catch (error: any) {
        processingSteps.userPageStatus = 'failed';
        if (!stepErrors.find(e => e.step === 'user_scraping')) {
          const stepError = errorHandler.createWorkerStepError(
            'user_scraping',
            'SCRAPING_FAILED',
            error.message,
            [],
            true  // Allow continuation with fallback
          );
          stepErrors.push(stepError);
        }
        
        logger.warn(`ğŸ”„ [WORKER] Exception during user page scraping, will use Gemini URL Context fallback:`, {
          url: userPageUrl,
          error: error.message
        });
        
        // Create fallback user page object for unexpected errors
        userPage = {
          url: userPageUrl,
          title: '',
          headings: [],
          cleanedContent: '', // Empty content signals fallback needed
          metaDescription: '',
          // No scrapeFailure property needed here, handled by stepErrors
        };
      }
      
      await job.updateProgress(40);

      // æº–å‚™ç«¶çˆ­è€… URL åˆ—è¡¨
      const searchReferences = aiOverview.references || [];
      const additionalCompetitorUrls = competitorUrls || [];
      const allCompetitorUrls = [
        ...searchReferences.slice(0, 10),
        ...additionalCompetitorUrls
      ];
      const uniqueCompetitorUrls = [...new Set(allCompetitorUrls)]
        .filter(url => url !== userPageUrl);

      // æ‰¹é‡çˆ¬å–ç«¶çˆ­è€…é é¢
      let competitorPages: PageContent[] = []; // Explicitly type competitorPages
      try {
        const competitorResults: ScrapeResult[] = await crawl4aiService.scrapeMultiplePages(uniqueCompetitorUrls); // Use crawl4aiService
        const successfulResults = competitorResults.filter((result: ScrapeResult) => result.success); // Explicitly type result
        const failedResults = competitorResults.filter((result: ScrapeResult) => !result.success); // Explicitly type result
        
        // è½‰æ›æˆåŠŸçš„çµæœç‚º PageContent æ ¼å¼
        competitorPages = successfulResults.map((result: ScrapeResult) => ({ // Explicitly type result
          url: result.url,
          title: result.title || '',
          headings: result.headings || [],
          cleanedContent: result.content || '',
          metaDescription: result.metaDescription || ''
        }));
        
        // è¨˜éŒ„å¤±æ•—çš„é é¢
        if (failedResults.length > 0) {
          logger.warn(`Job ${jobId}: ${failedResults.length}/${competitorResults.length} competitor pages failed to scrape`);
          failedResults.forEach((result: ScrapeResult) => { // Explicitly type result
            logger.warn(`Failed to scrape ${result.url}: ${result.error} - ${result.errorDetails}`);
          });
        }
        
        if (competitorPages.length === 0) {
          processingSteps.competitorPagesStatus = 'failed';
          logger.warn(`Job ${jobId}: No competitor pages successfully scraped`);
        } else if (failedResults.length > 0) {
          processingSteps.competitorPagesStatus = 'partial';
          completedSteps.push('competitor_scraping');
        } else {
          processingSteps.competitorPagesStatus = 'completed';
          completedSteps.push('competitor_scraping');
        }
      } catch (error: any) {
        logger.warn(`Job ${jobId}: Competitor scraping failed: ${error.message}`);
        processingSteps.competitorPagesStatus = 'failed';
        competitorPages = [];
      }

      

      // éšæ®µ 4: æœ€çµ‚å·®è·åˆ†æ (80-95%)
      finalPhaseStartTime = Date.now();
      logger.info(`Job ${jobId}: Performing content gap analysis`);
      processingSteps.aiAnalysisStatus = 'processing';

      let analysisResult: AnalysisReport;

      try {
        // Prepare crawled content for the prompt
        let crawledContent = '';
        const allScrapedPages = [userPage, ...competitorPages];

        allScrapedPages.forEach(page => {
          if (page.cleanedContent) {
            crawledContent += `--- URL: ${page.url} ---\n${page.cleanedContent}\n\n`;
          }
        });

        // Prepare cited URLs list for the prompt
        // const citedUrlsList = aiOverview.references.map(ref => `${ref}`).join('\n'); // Not needed

        logger.info(`Job ${jobId}: Preparing data for Gemini analysis`, {
          targetKeyword,
          userPageUrl: userPage.url,
          aiOverviewLength: aiOverview.summaryText?.length || 0,
          crawledContentLength: crawledContent.length,
          citedUrlsCount: aiOverview.references.length
        });

        // Construct GeminiInput object for the service
        const geminiInput = {
          targetKeyword,
          aiOverview,
          userPage,
          competitorPages,
          jobId
        };

        analysisResult = await geminiService.analyzeContentGap(geminiInput);

        processingSteps.aiAnalysisStatus = 'completed';
        completedSteps.push('ai_analysis');
      } catch (error: any) {
        logger.error(`Job ${jobId}: AI analysis failed: ${error.message}`);
        processingSteps.aiAnalysisStatus = 'failed';
        
        // æä¾›é™ç´šåˆ†æçµæœ
        analysisResult = this.generateFallbackAnalysis(targetKeyword, aiOverview);
      }

      await job.updateProgress(95);

      // éšæ®µ 5: çµæœæº–å‚™èˆ‡å„²å­˜ (95-100%)
      // è©•ä¼°ä»»å‹™å®Œæˆç‹€æ…‹
      const jobCompletion = errorHandler.evaluateJobCompletion(
        completedSteps,
        stepErrors,
        stepWarnings
      );

      // è¨˜éŒ„éŒ¯èª¤çµ±è¨ˆ
      if (stepErrors.length > 0 || stepWarnings.length > 0) {
        errorHandler.logErrorStatistics([...stepErrors, ...stepWarnings]);
      }

      const result = {
        ...analysisResult,
        analysisId: jobId,
        timestamp: new Date().toISOString(),
        aiOverviewData: {
          ...aiOverview,
          dataSource: aiOverview.fallbackUsed ? `${aiOverview.source} (fallback)` : 'AI Overview'
        },
        competitorUrls: uniqueCompetitorUrls,
        processingSteps,
        usedFallbackData: aiOverview.fallbackUsed || false,
        
        // v5.1 æ–°å¢ï¼šåˆ†å±¤éŒ¯èª¤è™•ç†çµæœ
        jobCompletion,
        errors: stepErrors.map(error => errorHandler.translateToFrontendError(error, jobId)),
        warnings: stepWarnings.map(warning => errorHandler.translateToFrontendError(warning, jobId)),
        qualityAssessment: {
          score: jobCompletion.qualityScore,
          level: this.getQualityLevel(jobCompletion.qualityScore),
          completedSteps: completedSteps.length,
          totalSteps: 4, // Changed from 5 to 4
          criticalFailures: stepErrors.filter(e => !e.canContinue).length,
          fallbacksUsed: jobCompletion.fallbacksUsed
        }
      };

      await job.updateProgress(100);

      const statusMessage = jobCompletion.status === 'completed' 
        ? 'completed successfully'
        : `completed with ${jobCompletion.status === 'completed_with_errors' ? 'errors' : 'issues'}`;
      
      const totalJobDuration = Date.now() - jobStartTime;
      const finalPhaseDuration = Date.now() - finalPhaseStartTime;
      
      logger.info(`ğŸ‰ [WORKER] Job completed`, {
        jobId,
        status: jobCompletion.status,
        statusMessage,
        qualityScore: jobCompletion.qualityScore,
        qualityLevel: this.getQualityLevel(jobCompletion.qualityScore),
        totalDuration: `${totalJobDuration}ms`,
        finalPhaseDuration: `${finalPhaseDuration}ms`,
        completedSteps: completedSteps.length,
        totalSteps: 4,
        errors: stepErrors.length,
        warnings: stepWarnings.length,
        phases: {
          serpApi: processingSteps.serpApiStatus,
          userPage: processingSteps.userPageStatus,
          competitorPages: processingSteps.competitorPagesStatus,
          aiAnalysis: processingSteps.aiAnalysisStatus
        }
      });
      
      return result;

    } catch (error: any) {
      const totalJobDuration = Date.now() - jobStartTime;
      logger.error(`âŒ [WORKER] Job failed`, {
        jobId,
        error: error.message,
        stack: error.stack,
        totalDuration: `${totalJobDuration}ms`,
        targetKeyword,
        userPageUrl
      });
      throw error;
    }
  }

  /**
   * ç”Ÿæˆé™ç´šåˆ†æçµæœï¼ˆClaudeAnalysisReport çµæ§‹ï¼‰
   */
  private generateFallbackAnalysis(targetKeyword: string, aiOverview: any): AnalysisReport {
    return {
      strategyAndPlan: {
        p1_immediate: [
          {
            recommendation: `âš ï¸ æŠ€è¡“å•é¡Œå°è‡´åˆ†æéƒ¨åˆ†å®Œæˆã€‚é—œéµå­—ã€Œ${targetKeyword}ã€çš„æœç´¢çµæœ${aiOverview.fallbackUsed ? 'ä½¿ç”¨äº†å‚™ç”¨æ•¸æ“šæº' : 'åŒ…å« AI Overview'}ï¼Œä½†ç„¡æ³•å®Œæˆå®Œæ•´çš„ AI åˆ†æã€‚`,
            geminiPrompt: "è«‹æª¢æŸ¥ç³»çµ±è¨­å®šä¸¦é‡æ–°å˜—è©¦åˆ†æï¼Œæˆ–æ‰‹å‹•æª¢æŸ¥ç«¶çˆ­å°æ‰‹å…§å®¹ã€‚"
          }
        ],
        p2_mediumTerm: [],
        p3_longTerm: []
      },
      keywordIntent: {
        coreIntent: "åˆ†æå¤±æ•—ï¼Œç„¡æ³•ç¢ºå®šæœç´¢æ„åœ–",
        latentIntents: ["å»ºè­°æ‰‹å‹•æª¢æŸ¥ç«¶çˆ­å°æ‰‹", "é‡æ–°å˜—è©¦ç³»çµ±åˆ†æ"]
      },
      aiOverviewAnalysis: {
        summary: "AI Overview åˆ†æå› æŠ€è¡“å•é¡Œç„¡æ³•å®Œæˆ",
        presentationAnalysis: "ç„¡æ³•åˆ†æå‘ˆç¾æ–¹å¼ï¼Œå»ºè­°æ‰‹å‹•æª¢æŸ¥"
      },
      citedSourceAnalysis: [],
      websiteAssessment: {
        contentSummary: "åˆ†æå¤±æ•—ï¼Œç„¡æ³•è©•ä¼°ç¶²ç«™å…§å®¹",
        contentGaps: [`ç¼ºå°‘é‡å° ${targetKeyword} çš„æ·±åº¦å…§å®¹`],
        pageExperience: "ç„¡æ³•è©•ä¼°é é¢é«”é©—",
        structuredDataRecs: "å»ºè­°æ·»åŠ åŸºæœ¬çš„ Schema æ¨™è¨˜"
      },
      reportFooter: "âš ï¸ æœ¬å ±å‘Šå› æŠ€è¡“å•é¡Œè€Œç°¡åŒ–ç”Ÿæˆã€‚å»ºè­°é‡æ–°å˜—è©¦åˆ†æä»¥ç²å¾—å®Œæ•´çµæœï¼Œæˆ–è¯ç¹«æŠ€è¡“æ”¯æŒç²å¾—å”åŠ©ã€‚"
    };
  }


  /**
   * ç²å–å“è³ªç­‰ç´šï¼ˆæ–°ç‰ˆæœ¬ï¼‰
   */
  private getQualityLevel(score: number): 'excellent' | 'good' | 'fair' | 'poor' {
    if (score >= 85) return 'excellent';
    if (score >= 70) return 'good';
    if (score >= 50) return 'fair';
    return 'poor';
  }

  /**
   * å„ªé›…é—œé–‰ Worker
   */
  async shutdown(): Promise<void> {
    try {
      await this.worker.close();
      await this.redisConnection.quit();
      logger.info('Analysis worker shutdown completed');
    } catch (error) {
      logger.error('Error during worker shutdown:', error);
    }
  }
}

// å‰µå»ºä¸¦å•Ÿå‹• Worker
const analysisWorker = new AnalysisWorker();

logger.info('Analysis Worker initialized successfully');

export default analysisWorker;